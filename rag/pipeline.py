# -*- coding: utf-8 -*-
"""Lab4_new.ipynb
THIS NEEDS TO BE CLEANED UP FOR GITHUB
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwXCIg81UGuVSNn9ruBYSe2g0SqBWMPK
"""

!pip -q install pymupdf scikit-learn pillow streamlit

import os, re, glob, json, time, csv
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime, timezone

import numpy as np
import pandas as pd

import fitz
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize

print("✅ Imports loaded.")

# --- Lab 4 repo-style paths ---
DATA_DIR = "/content/data"
PDF_DIR  = os.path.join(DATA_DIR, "pdfs")
FIG_DIR  = os.path.join(DATA_DIR, "figures")
LOGS_DIR = "./logs"
LOG_FILE = os.path.join(LOGS_DIR, "query_metrics.csv")

# Retrieval knobs (carry over from Lab 3)
TOP_K_TEXT     = 5
TOP_K_IMAGES   = 3
TOP_K_EVIDENCE = 8
ALPHA          = 0.5  # 0.0=images dominate, 1.0=text dominates

# Optional chunking knobs (only needed if you still want fixed-chunk ablation)
CHUNK_SIZE    = 900
CHUNK_OVERLAP = 150

RANDOM_SEED = 0
np.random.seed(RANDOM_SEED)

os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs(FIG_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)

print("✅ Config set.")
print("PDF_DIR:", PDF_DIR)
print("FIG_DIR:", FIG_DIR)
print("LOG_FILE:", LOG_FILE)

# LAB 4 — CELL 2 (Your Project Mini-Gold Set Q1–Q5)
# (UseLab 3 Q1–Q3 + add Q4 multimodal + Q5 missing)
MISSING_EVIDENCE_MSG = "Not enough evidence in the retrieved context."

mini_gold = [
    {
        "query_id": "Q1",
        "question": "What is the overall SQLENS pipeline and what happens in each step?",
        "gold_evidence_ids": []
    },
    {
        "query_id": "Q2",
        "question": "What semantic error types are shown in the causal graph and what signals are used to detect them?",
        "gold_evidence_ids": []
    },
    {
        "query_id": "Q3",
        "question": "How does FACT reduce inconsistent hallucinations, and what kinds of hallucinations does it target?",
        "gold_evidence_ids": []
    },
    {
        "query_id": "Q4",
        "question": "Using the figure of the SQLENS pipeline, list the pipeline stages in order.",
        # fill this w/ screenshot filename once confirmed
        #like  ["Screenshot 2026-02-12 085908.png"]
        "gold_evidence_ids": []
    },
    {
        "query_id": "Q5",
        "question": "Who won the FIFA World Cup in 2050?",
        "gold_evidence_ids": ["N/A"]
    },
]

pd.DataFrame(mini_gold)

caption_map = {
    "Screenshot 2026-02-12 085908.png": "SQLENS pipeline: Error Detector -> Error Selector -> Error Fixer -> SQL Auditor",
    "Screenshot 2026-02-12 085920.png": "Causal graph: semantic errors and DB/LLM signals (ambiguity, evidence violation, join predicate, join tree)",
    "Screenshot 2026-02-12 085933.png": "Signal aggregation via weak supervision: labeling functions -> generative model -> correctness prediction",
    "Screenshot 2026-02-12 090033.png": "ALIGNRAG overview: retrieval + critique synthesis + critique-driven alignment",
    "Screenshot 2026-02-12 090118.png": "FACT example: input/context conflicting hallucinations and correction",
    "Screenshot 2026-02-12 090126.png": "FACT overview: filtering fact text + alternating code-text training + quality assessment",
    "Screenshot 2026-02-12 090137.png": "FACT simplified example: text segment paired with code representation",
}
print("✅ caption_map entries:", len(caption_map))

print("Checking folders...")

print("PDF_DIR exists:", os.path.isdir(PDF_DIR))
print("FIG_DIR exists:", os.path.isdir(FIG_DIR))

print("PDF files:", os.listdir(PDF_DIR))
print("Figure files:", os.listdir(FIG_DIR))

#Lab 3 ingestion, fixed for Lab 4 paths
from dataclasses import dataclass

@dataclass
class TextChunk:
    chunk_id: str
    doc_id: str
    page_num: int
    text: str

@dataclass
class ImageItem:
    item_id: str
    path: str
    caption: str

def clean_text(s: str) -> str:
    s = s or ""
    return re.sub(r"\s+", " ", s).strip()

def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:
    doc_id = os.path.basename(pdf_path)
    doc = fitz.open(pdf_path)
    out: List[TextChunk] = []
    for i in range(len(doc)):
        page = doc.load_page(i)
        text = clean_text(page.get_text("text"))
        if text:
            out.append(TextChunk(
                chunk_id=f"{doc_id}::p{i+1}",
                doc_id=doc_id,
                page_num=i+1,
                text=text
            ))
    return out

def load_images(fig_dir: str) -> List[ImageItem]:
    items: List[ImageItem] = []
    for p in sorted(glob.glob(os.path.join(fig_dir, "*.*"))):
        if not p.lower().endswith((".png", ".jpg", ".jpeg", ".webp")):
            continue
        base = os.path.basename(p)
        caption = os.path.splitext(base)[0].replace("_", " ")
        items.append(ImageItem(item_id=base, path=p, caption=caption))
    return items

# Find PDFs (this fixes your earlier "pdfs not defined" issue)
pdfs = sorted(glob.glob(os.path.join(PDF_DIR, "*.pdf")))
print("✅ PDFs found:", len(pdfs))
for p in pdfs[:10]:
    print(" -", os.path.basename(p))

# Extract pages
page_chunks: List[TextChunk] = []
for p in pdfs:
    page_chunks.extend(extract_pdf_pages(p))

# Load images
image_items = load_images(FIG_DIR)

# Apply caption_map
if caption_map:
    updated = 0
    for it in image_items:
        if it.item_id in caption_map:
            it.caption = caption_map[it.item_id]
            updated += 1
    print("✅ Updated captions for:", updated, "images")
else:
    print("ℹ️ No caption_map provided; using filename-based captions.")

print("✅ Ingestion complete.")
print("Total text chunks:", len(page_chunks))
print("Total images:", len(image_items))
print("Sample text:", page_chunks[0].chunk_id, page_chunks[0].text[:140] if page_chunks else "NONE")
print("Sample image:", image_items[0] if image_items else "NONE")

assert len(page_chunks) > 0 or len(image_items) > 0, (
    "No data found. Put PDFs in ./data/pdfs and screenshots/images in ./data/figures."
)

pdfs = sorted(glob.glob(os.path.join(PDF_DIR, "*.pdf")))
print("PDFs found:", len(pdfs))
for p in pdfs:
    print(os.path.basename(p))

import fitz, os

for p in pdfs:
    doc = fitz.open(p)
    doc_id = os.path.basename(p)
    print("\n===", doc_id, "pages:", len(doc), "===")
    for i in range(len(doc)):
        print(f"{doc_id}::p{i+1}")

def find_pages_containing(term: str):
    term = term.lower()
    hits = []
    for ch in page_chunks:
        if term in ch.text.lower():
            hits.append(ch.chunk_id)
    return hits[:30]  # show first 30

print(find_pages_containing("SQLENS"))
print(find_pages_containing("Error Detector"))
print(find_pages_containing("FACT"))

def build_tfidf_index_text(chunks: List[TextChunk]):
    corpus = [c.text for c in chunks]
    vec = TfidfVectorizer(lowercase=True, stop_words="english")
    X = vec.fit_transform(corpus)
    X = normalize(X)
    return vec, X

def build_tfidf_index_images(items: List[ImageItem]):
    corpus = [it.caption for it in items]
    vec = TfidfVectorizer(lowercase=True, stop_words="english")
    X = vec.fit_transform(corpus)
    X = normalize(X)
    return vec, X

def tfidf_retrieve(query: str, vec: TfidfVectorizer, X, top_k: int = 5):
    q = vec.transform([query])
    q = normalize(q)
    scores = (X @ q.T).toarray().ravel()
    idx = np.argsort(-scores)[:top_k]
    return [(int(i), float(scores[i])) for i in idx]

text_vec, text_X = build_tfidf_index_text(page_chunks) if page_chunks else (None, None)
img_vec, img_X   = build_tfidf_index_images(image_items) if image_items else (None, None)

print("✅ Indexes built.")
print("Text index:", "OK" if text_vec is not None else "NONE")
print("Image index:", "OK" if img_vec is not None else "NONE")

def _normalize_scores(pairs):
    if not pairs:
        return []
    scores = [s for _, s in pairs]
    lo, hi = min(scores), max(scores)
    if abs(hi - lo) < 1e-12:
        return [(i, 1.0) for i, _ in pairs]
    return [(i, (s - lo) / (hi - lo)) for i, s in pairs]

def build_context(
    question: str,
    top_k_text: int = TOP_K_TEXT,
    top_k_images: int = TOP_K_IMAGES,
    top_k_evidence: int = TOP_K_EVIDENCE,
    alpha: float = ALPHA,
    use_multimodal: bool = True,
) -> Dict[str, Any]:
    text_hits = []
    if text_vec is not None and text_X is not None:
        text_hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k_text)

    img_hits = []
    if use_multimodal and img_vec is not None and img_X is not None:
        img_hits = tfidf_retrieve(question, img_vec, img_X, top_k=top_k_images)

    text_norm = _normalize_scores(text_hits)
    img_norm  = _normalize_scores(img_hits)

    fused = []

    # Text evidence
    for idx, s in text_norm:
        ch = page_chunks[idx]
        fused.append({
            "modality": "text",
            "id": ch.chunk_id,                  # evidence id
            "raw_score": float(dict(text_hits).get(idx, 0.0)),
            "fused_score": float(alpha * s),
            "text": ch.text,
            "path": None,
            "citation_tag": f"[{ch.chunk_id}]",
        })

    # Image evidence (caption surrogate)
    for idx, s in img_norm:
        it = image_items[idx]
        fused.append({
            "modality": "image",
            "id": it.item_id,                   # evidence id (filename)
            "raw_score": float(dict(img_hits).get(idx, 0.0)),
            "fused_score": float((1.0 - alpha) * s),
            "text": it.caption,
            "path": it.path,
            "citation_tag": f"[{it.item_id}]",
        })

    fused = sorted(fused, key=lambda d: d["fused_score"], reverse=True)[:top_k_evidence]

    ctx_lines = []
    image_paths = []
    for ev in fused:
        if ev["modality"] == "text":
            snippet = (ev["text"] or "")[:260].replace("\n", " ")
            ctx_lines.append(f"{ev['citation_tag']} {snippet}")
        else:
            ctx_lines.append(f"{ev['citation_tag']} caption={ev['text']}")
            image_paths.append(ev["path"])

    return {
        "question": question,
        "context": "\n".join(ctx_lines),
        "image_paths": image_paths,
        "evidence": fused,
        "alpha": alpha
    }

# Demo
demo = build_context(mini_gold[0]["question"], use_multimodal=True)
print(demo["context"][:1200])
print("Images:", [os.path.basename(p) for p in demo["image_paths"]])

def simple_extractive_answer(context: str, n_lines: int = 3) -> str:
    lines = [ln for ln in context.splitlines() if ln.strip()]
    if not lines:
        return MISSING_EVIDENCE_MSG
    return "\n".join(lines[:n_lines])

def generate_answer(question: str, ctx: Dict[str, Any]) -> str:
    # Lab 4: missing-evidence behavior
    if not ctx["evidence"]:
        return MISSING_EVIDENCE_MSG

    # If scores are basically zero-ish, treat as missing
    best = max(ev.get("fused_score", 0.0) for ev in ctx["evidence"])
    if best < 0.05:
        return MISSING_EVIDENCE_MSG

    # Extractive baseline (guaranteed “grounded”)
    answer = simple_extractive_answer(ctx["context"], n_lines=3)
    return answer

# Quick test
ctx = build_context(mini_gold[0]["question"], use_multimodal=True)
ans = generate_answer(mini_gold[0]["question"], ctx)
print("Answer:\n", ans)

def run_pipeline(question: str, retrieval_mode: str = "mm", top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA):
    use_multimodal = (retrieval_mode == "mm")
    ctx = build_context(
        question,
        top_k_text=top_k_text,
        top_k_images=top_k_images,
        top_k_evidence=top_k_evidence,
        alpha=alpha,
        use_multimodal=use_multimodal
    )
    answer = generate_answer(question, ctx)
    return answer, ctx

# demo compare modes
q = mini_gold[0]["question"]
a1, c1 = run_pipeline(q, retrieval_mode="mm")
a2, c2 = run_pipeline(q, retrieval_mode="text_only")
print("=== mm ===\n", a1[:600], "\n")
print("=== text_only ===\n", a2[:600])

def ensure_logfile(path: str):
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    if not p.exists():
        header = [
            "timestamp", "query_id", "retrieval_mode", "top_k_evidence", "latency_ms",
            "Precision@5", "Recall@10",
            "evidence_ids_returned", "gold_evidence_ids",
            "faithfulness_pass", "missing_evidence_behavior"
        ]
        with open(p, "w", newline="", encoding="utf-8") as f:
            csv.writer(f).writerow(header)

ensure_logfile(LOG_FILE)

def precision_at_k_ids(retrieved_ids: List[str], gold_ids: List[str], k: int = 5):
    if not gold_ids or gold_ids == ["N/A"]:
        return np.nan
    topk = retrieved_ids[:k]
    return len(set(topk) & set(gold_ids)) / float(k)

def recall_at_k_ids(retrieved_ids: List[str], gold_ids: List[str], k: int = 10):
    if not gold_ids or gold_ids == ["N/A"]:
        return np.nan
    topk = retrieved_ids[:k]
    denom = max(1, len(set(gold_ids)))
    return len(set(topk) & set(gold_ids)) / float(denom)

def faithfulness_heuristic(answer: str, evidence: List[Dict[str, Any]]):
    # Pass if missing-evidence msg OR answer contains at least one citation tag from top evidence lines
    if answer.strip() == MISSING_EVIDENCE_MSG:
        return True
    tags = [ev.get("citation_tag","") for ev in evidence[:5]]
    return any(t and t in answer for t in tags)

def missing_evidence_behavior(answer: str, evidence: List[Dict[str, Any]]):
    has_ev = bool(evidence) and max(ev.get("fused_score", 0.0) for ev in evidence) >= 0.05
    if not has_ev:
        return "Pass" if answer.strip() == MISSING_EVIDENCE_MSG else "Fail"
    return "Pass" if answer.strip() != MISSING_EVIDENCE_MSG else "Fail"

def run_query_and_log(query_item: Dict[str, Any], retrieval_mode: str = "mm"):
    question = query_item["question"]
    gold_ids = query_item.get("gold_evidence_ids", [])

    t0 = time.time()
    answer, ctx = run_pipeline(question, retrieval_mode=retrieval_mode)
    latency_ms = (time.time() - t0) * 1000.0

    evidence_ids = [ev["id"] for ev in ctx["evidence"]]
    p5  = precision_at_k_ids(evidence_ids, gold_ids, k=5)
    r10 = recall_at_k_ids(evidence_ids, gold_ids, k=10)

    faithful = faithfulness_heuristic(answer, ctx["evidence"])
    meb = missing_evidence_behavior(answer, ctx["evidence"])

    row = [
        datetime.now(timezone.utc).isoformat(),
        query_item["query_id"],
        retrieval_mode,
        TOP_K_EVIDENCE,
        round(latency_ms, 2),
        p5,
        r10,
        json.dumps(evidence_ids),
        json.dumps(gold_ids),
        "Yes" if faithful else "No",
        meb
    ]
    with open(LOG_FILE, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(row)

    return {"answer": answer, "ctx": ctx, "p5": p5, "r10": r10, "latency_ms": latency_ms, "faithful": faithful, "meb": meb}

# Run the gold set once in BOTH modes (this is what Lab 4 wants in results)
outs = []
for qi in mini_gold:
    outs.append(run_query_and_log(qi, retrieval_mode="mm"))
for qi in mini_gold:
    outs.append(run_query_and_log(qi, retrieval_mode="text_only"))

pd.read_csv(LOG_FILE).tail(12)

streamlit_code = r'''
import json, time
from pathlib import Path
import streamlit as st
import pandas as pd

# NOTE: This app expects the same folder structure:
# ./data/pdfs, ./data/figures, ./logs

MISSING_EVIDENCE_MSG = "Not enough evidence in the retrieved context."

# --- import pipeline from rag/pipeline.py if you later refactor ---
# For now (simple): we load a saved CSV logger only and call an API later if you do extension

st.set_page_config(page_title="CS5542 Lab 4 — Project RAG App", layout="wide")
st.title("CS 5542 Lab 4 — Project RAG Application")

st.sidebar.header("Retrieval Settings")
retrieval_mode = st.sidebar.selectbox("retrieval_mode", ["mm", "text_only"])
st.sidebar.caption("mm = multimodal TF-IDF fusion; text_only = text pages only")

st.sidebar.header("Logging")
log_path = st.sidebar.text_input("log file", value="logs/query_metrics.csv")

MINI_GOLD = {
    "Q1": "What is the overall SQLENS pipeline and what happens in each step?",
    "Q2": "What semantic error types are shown in the causal graph and what signals are used to detect them?",
    "Q3": "How does FACT reduce inconsistent hallucinations, and what kinds of hallucinations does it target?",
    "Q4": "Using the figure of the SQLENS pipeline, list the pipeline stages in order.",
    "Q5": "Who won the FIFA World Cup in 2050?"
}

query_id = st.sidebar.selectbox("query_id", list(MINI_GOLD.keys()))
use_gold_question = st.sidebar.checkbox("Use gold question text", value=True)

default_q = MINI_GOLD[query_id] if use_gold_question else ""
question = st.text_area("Enter your question", value=default_q, height=120)
run_btn = st.button("Run Query")

st.info("This starter app is UI-only. For full Lab 4, connect it to your pipeline by importing functions from a /rag module or by adding a FastAPI backend.")

if run_btn and question.strip():
    st.warning("Next step: move your notebook pipeline into /rag/pipeline.py and import it here.")
    st.write("Question:", question)
    st.write("Selected mode:", retrieval_mode)
    st.write("Logging file:", log_path)
'''

app_dir = Path("app")
app_dir.mkdir(parents=True, exist_ok=True)
(app_dir / "main.py").write_text(streamlit_code, encoding="utf-8")
print("✅ Wrote Streamlit app stub to:", app_dir / "main.py")
print("\nTo run locally:")
print("  streamlit run app/main.py")


